{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5tv2AAO7pXbQf6aaOizIH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SankethHanasi/6thSem-ML-Lab/blob/main/1BM22CS242_Lab2_ID3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dUDmeyoDsVw",
        "outputId": "0bd0cfc2-dc9b-4b65-f993-56be85e433f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entropy and Information Gain for each feature:\n",
            "Feature: Outlook | Entropy: 0.9403 | Information Gain: 0.2467\n",
            "Feature: Temperature | Entropy: 0.9403 | Information Gain: 0.0292\n",
            "Feature: Humidity | Entropy: 0.9403 | Information Gain: 0.1518\n",
            "Feature: Wind | Entropy: 0.9403 | Information Gain: 0.0481\n",
            "\n",
            "Decision Tree:\n",
            "    Outlook:\n",
            "      Sunny ->         Humidity:\n",
            "          High ->             No\n",
            "          Normal ->             Yes\n",
            "      Overcast ->         Yes\n",
            "      Rainy ->         Wind:\n",
            "          Weak ->             Yes\n",
            "          Strong ->             No\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample weather dataset\n",
        "data = {\n",
        "    'Day': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
        "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy'],\n",
        "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
        "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
        "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
        "    'Decision': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Function to calculate entropy\n",
        "def entropy(target):\n",
        "    # Get the counts of each class\n",
        "    class_counts = target.value_counts()\n",
        "    # Calculate the entropy using the formula\n",
        "    probabilities = class_counts / len(target)\n",
        "    return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "# Function to calculate information gain\n",
        "def information_gain(data, feature, target):\n",
        "    # Calculate the entropy of the whole dataset\n",
        "    entropy_before = entropy(target)\n",
        "\n",
        "    # Get the unique values of the feature\n",
        "    feature_values = data[feature].unique()\n",
        "\n",
        "    # Calculate the weighted entropy after the split\n",
        "    weighted_entropy = 0\n",
        "    for value in feature_values:\n",
        "        subset = target[data[feature] == value]\n",
        "        weighted_entropy += (len(subset) / len(target)) * entropy(subset)\n",
        "\n",
        "    # Information gain is the reduction in entropy\n",
        "    return entropy_before - weighted_entropy\n",
        "\n",
        "# Function to print entropy and information gain for each feature\n",
        "def print_entropy_and_gain(data, features, target):\n",
        "    print(\"\\nEntropy and Information Gain for each feature:\")\n",
        "    for feature in features:\n",
        "        gain = information_gain(data, feature, target)\n",
        "        ent = entropy(target)\n",
        "        print(f\"Feature: {feature} | Entropy: {ent:.4f} | Information Gain: {gain:.4f}\")\n",
        "\n",
        "# Function to build the decision tree recursively\n",
        "def build_tree(data, target, features):\n",
        "    # Base case: If all target values are the same, return a leaf node\n",
        "    if len(target.unique()) == 1:\n",
        "        return target.iloc[0]\n",
        "\n",
        "    # Base case: If no features left to split, return the majority class\n",
        "    if len(features) == 0:\n",
        "        return target.mode()[0]\n",
        "\n",
        "    # Calculate information gain for each feature\n",
        "    gains = {feature: information_gain(data, feature, target) for feature in features}\n",
        "\n",
        "    # Find the feature with the highest information gain\n",
        "    best_feature = max(gains, key=gains.get)\n",
        "\n",
        "    # Create the tree node with the best feature\n",
        "    tree = {best_feature: {}}\n",
        "\n",
        "    # Get the unique values of the best feature\n",
        "    feature_values = data[best_feature].unique()\n",
        "\n",
        "    # Recursively build the tree for each subset of the data\n",
        "    for value in feature_values:\n",
        "        subset_data = data[data[best_feature] == value]\n",
        "        subset_target = target[data[best_feature] == value]\n",
        "\n",
        "        # Remove the best feature from the list of features for the next level\n",
        "        remaining_features = [f for f in features if f != best_feature]\n",
        "\n",
        "        # Build the subtree for the subset\n",
        "        subtree = build_tree(subset_data, subset_target, remaining_features)\n",
        "\n",
        "        # Add the subtree to the tree\n",
        "        tree[best_feature][value] = subtree\n",
        "\n",
        "    return tree\n",
        "\n",
        "# Function to print the tree in a visually structured way\n",
        "def print_tree(tree, indent=\"\"):\n",
        "    if isinstance(tree, dict):\n",
        "        for feature, branches in tree.items():\n",
        "            print(f\"{indent}{feature}:\")\n",
        "            for value, subtree in branches.items():\n",
        "                print(f\"{indent}  {value} ->\", end=\" \")\n",
        "                print_tree(subtree, indent + \"    \")\n",
        "    else:\n",
        "        print(f\"{indent}{tree}\")\n",
        "\n",
        "# Target variable\n",
        "target = df['Decision']\n",
        "\n",
        "# Features\n",
        "features = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
        "\n",
        "# Step 1: Print entropy and information gain for each feature\n",
        "print_entropy_and_gain(df, features, target)\n",
        "\n",
        "# Step 2: Build the decision tree\n",
        "tree = build_tree(df, target, features)\n",
        "\n",
        "# Step 3: Print the decision tree (formatted)\n",
        "print(\"\\nDecision Tree:\")\n",
        "print_tree(tree, indent=\"    \")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZEQSHpHOGJkq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}